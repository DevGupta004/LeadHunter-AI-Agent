"""
Google Maps AI-Powered Scraper
Uses Local Llama 3.2 for intelligent data extraction
Hybrid approach: Playwright + AI for best results
"""

import streamlit as st
from playwright.sync_api import sync_playwright
import time
import json
import requests
import re

st.title("ü§ñ AI-Powered Google Maps Scraper")
st.caption("Using Local Llama 3.2 for Intelligent Extraction")

# Configuration
url = st.text_input(
    "Google Maps URL",
    value="https://www.google.com/maps/search/cloth+stores+in+Lucknow/",
)

col1, col2 = st.columns(2)
with col1:
    max_stores = st.slider("Max stores", 5, 50, 10)
with col2:
    enable_pagination = st.checkbox("Enable pagination", value=True)

st.info("""
ü§ñ **AI-Powered Features:**
- ‚úÖ Uses Local Llama 3.2 (no API needed!)
- üß† Understands context and variations
- üéØ Smarter data extraction
- üîÑ Auto-adapts to format changes
- ‚ö° Falls back to regex for speed

**Expected:** 60-80% accuracy (better than pure regex!)
""")

# AI Helper Functions
def call_ollama_ai(prompt, model="llama3.2"):
    """Call local Ollama AI for intelligent extraction"""
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0,
                    "num_predict": 100
                }
            },
            timeout=30
        )
        if response.status_code == 200:
            return response.json()["response"].strip()
        return None
    except Exception as e:
        st.warning(f"AI call failed: {str(e)}")
        return None

def extract_with_ai(text, field_name):
    """Use AI to extract specific field from text"""
    prompt = f"""From this Google Maps store text, extract ONLY the {field_name}.
Return just the value, nothing else. If not found, return "Not found".

Text: {text[:1500]}

{field_name}:"""
    
    result = call_ollama_ai(prompt)
    return result if result else "Not found"

def extract_all_fields_with_ai(text):
    """Use AI to extract all fields at once (more efficient)"""
    prompt = f"""From this Google Maps store page, extract the following information.
Return ONLY a JSON object with these exact fields:

{{
  "store_name": "name here",
  "rating": "4.5 or N/A",
  "reviews_count": "number or N/A",
  "phone": "phone number or Not found",
  "address": "full address or Not found",
  "hours": "opening hours or Not found",
  "website": "website URL or Not found"
}}

Text from page:
{text[:2000]}

JSON:"""
    
    result = call_ollama_ai(prompt)
    if result:
        try:
            # Try to parse JSON
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
        except:
            pass
    return None

def extract_coords_from_url(url):
    """Extract coordinates from URL (fast, no AI needed)"""
    coords_match = re.search(r'@(-?\d+\.\d+),(-?\d+\.\d+)', url)
    if coords_match:
        return coords_match.group(1), coords_match.group(2)
    return None, None

def extract_phone_fallback(text):
    """Regex fallback for phone (fast)"""
    patterns = [
        r'\d{5}\s?\d{5}',
        r'\+91[\s-]?\d{10}',
        r'0\d{2,4}[\s-]?\d{6,8}',
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    return None

# Check if Ollama is running
def check_ollama():
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            return True
    except:
        pass
    return False

# Main UI
if st.button("üöÄ Start AI-Powered Scraping", type="primary"):
    
    # Check Ollama
    if not check_ollama():
        st.error("""
        ‚ùå **Ollama is not running!**
        
        Please start Ollama first:
        ```bash
        ollama serve
        ```
        
        Make sure Llama 3.2 is installed:
        ```bash
        ollama pull llama3.2
        ```
        """)
        st.stop()
    
    st.success("‚úÖ Ollama is running!")
    
    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        with sync_playwright() as p:
            status_text.text("üåê Launching browser...")
            
            browser = p.chromium.launch(
                headless=False,
                slow_mo=500
            )
            
            page = browser.new_page(viewport={'width': 1400, 'height': 900})
            
            status_text.text("üìç Loading Google Maps...")
            page.goto(url, timeout=30000)
            time.sleep(15)
            
            # Pagination
            if enable_pagination:
                status_text.text("üìú Loading more results...")
                for scroll in range(max(3, max_stores // 10)):
                    page.evaluate('''
                        const feed = document.querySelector('[role="feed"]');
                        if (feed) feed.scrollTo(0, feed.scrollHeight);
                    ''')
                    time.sleep(2)
                time.sleep(3)
            
            # Find stores
            status_text.text("üîç Finding stores...")
            selectors = ['a[href*="/maps/place/"]', '[role="article"]']
            
            store_elements = []
            for selector in selectors:
                elements = page.query_selector_all(selector)
                if len(elements) > 0:
                    store_elements = elements[:max_stores]
                    st.success(f"‚úÖ Found {len(elements)} stores")
                    break
            
            if not store_elements:
                st.error("‚ùå No stores found")
                browser.close()
                st.stop()
            
            # Process each store
            for i in range(min(max_stores, len(store_elements))):
                try:
                    progress_bar.progress((i + 1) / min(max_stores, len(store_elements)))
                    status_text.text(f"ü§ñ AI analyzing store {i+1}/{min(max_stores, len(store_elements))}...")
                    
                    # Re-find store elements to avoid stale references
                    current_elements = page.query_selector_all(selectors[0])
                    if i >= len(current_elements):
                        st.warning(f"‚ö†Ô∏è Store {i+1} not found, skipping...")
                        continue
                    
                    element = current_elements[i]
                    
                    # Scroll element into view
                    try:
                        element.scroll_into_view_if_needed()
                        time.sleep(0.5)
                    except:
                        pass
                    
                    # Store the old URL to detect change
                    old_url = page.url
                    
                    # Click store
                    try:
                        element.click()
                    except Exception as click_error:
                        st.warning(f"‚ö†Ô∏è Could not click store {i+1}: {str(click_error)}")
                        continue
                    
                    # Wait for URL to change (indicates new store loaded)
                    for wait_attempt in range(15):  # Try for 15 seconds
                        time.sleep(1)
                        if page.url != old_url:
                            break
                    
                    # Extra wait for content to fully load
                    time.sleep(5)
                    
                    # Wait for store name element to be visible
                    try:
                        page.wait_for_selector('h1', timeout=10000)
                    except:
                        pass
                    
                    # Get FRESH page data
                    current_url = page.url
                    page_text = page.inner_text('body')
                    
                    # Debug: Show URL change
                    if current_url == old_url:
                        st.warning(f"‚ö†Ô∏è Store {i+1}: URL didn't change! Might be duplicate data.")
                    
                    # Extract coordinates (fast, no AI needed)
                    lat, lng = extract_coords_from_url(current_url)
                    
                    # Try AI extraction first
                    status_text.text(f"üß† AI extracting data for store {i+1}...")
                    ai_data = extract_all_fields_with_ai(page_text)
                    
                    if ai_data:
                        # AI succeeded
                        store_name = ai_data.get('store_name', 'Unknown')
                        rating = ai_data.get('rating', 'N/A')
                        reviews = ai_data.get('reviews_count', 'N/A')
                        phone = ai_data.get('phone', 'Not found')
                        address = ai_data.get('address', 'Not found')
                        hours = ai_data.get('hours', 'Not found')
                        website = ai_data.get('website', 'Not found')
                    else:
                        # AI failed, fallback to regex
                        status_text.text(f"‚ö° Using regex fallback for store {i+1}...")
                        
                        # Basic extraction
                        name_el = page.query_selector('h1')
                        store_name = name_el.inner_text() if name_el else "Unknown"
                        
                        rating_match = re.search(r'(\d\.\d)', page_text)
                        rating = rating_match.group(1) if rating_match else 'N/A'
                        
                        review_match = re.search(r'\((\d+(?:,\d+)?)\)', page_text)
                        reviews = review_match.group(1).replace(',', '') if review_match else 'N/A'
                        
                        phone = extract_phone_fallback(page_text)
                        if not phone:
                            phone = 'Not found'
                        
                        address = 'Not found'
                        hours = 'Not found'
                        website = 'Not found'
                    
                    # Extract Plus Code (regex is fine)
                    plus_code = None
                    plus_match = re.search(r'[A-Z0-9]{4}\+[A-Z0-9]{2,3}\s+\w+', page_text)
                    if plus_match:
                        plus_code = plus_match.group(0)
                    
                    # Build result
                    result = {
                        'number': i + 1,
                        'store_name': store_name,
                        'rating': rating,
                        'reviews_count': reviews,
                        'phone_number': phone,
                        'address': address,
                        'opening_hours': hours,
                        'website': website,
                        'plus_code': plus_code if plus_code else 'Not found',
                        'latitude': lat if lat else 'Not found',
                        'longitude': lng if lng else 'Not found',
                        'google_maps_url': current_url,
                        'extraction_method': 'AI' if ai_data else 'Regex'
                    }
                    
                    results.append(result)
                    
                    # Show result
                    method_icon = "ü§ñ" if ai_data else "‚ö°"
                    with st.expander(f"{method_icon} {store_name} - ‚≠ê {rating}", expanded=False):
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"**Extraction:** {result['extraction_method']}")
                            st.write(f"**‚≠ê Rating:** {rating} ({reviews} reviews)")
                            st.write(f"**üìû Phone:** {phone}")
                            st.write(f"**üïê Hours:** {hours}")
                        with col2:
                            st.write(f"**üìç Address:** {address[:80]}...")
                            st.write(f"**üåê Website:** {website}")
                            st.write(f"**üåç Coords:** {lat}, {lng}")
                    
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Error with store {i+1}: {str(e)}")
                    continue
            
            browser.close()
            
            # Summary
            st.success(f"‚úÖ Completed! Scraped {len(results)} stores")
            
            # Statistics
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                ai_count = sum(1 for r in results if r['extraction_method'] == 'AI')
                st.metric("ü§ñ AI Extracted", f"{ai_count}/{len(results)}")
            with col2:
                phones = sum(1 for r in results if r['phone_number'] != 'Not found')
                st.metric("üìû Phones", f"{phones}/{len(results)}")
            with col3:
                ratings = sum(1 for r in results if r['rating'] != 'N/A')
                st.metric("‚≠ê Ratings", f"{ratings}/{len(results)}")
            with col4:
                addresses = sum(1 for r in results if r['address'] != 'Not found')
                st.metric("üìç Addresses", f"{addresses}/{len(results)}")
            
            # Download
            if results:
                st.download_button(
                    "üì• Download Results (JSON)",
                    data=json.dumps(results, indent=2, ensure_ascii=False),
                    file_name="google_maps_ai_results.json",
                    mime="application/json"
                )
            
    except Exception as e:
        st.error(f"‚ùå Error: {str(e)}")

# Info Section
st.markdown("---")

col1, col2 = st.columns(2)

with col1:
    st.subheader("ü§ñ AI-Powered Features")
    st.markdown("""
    **What AI does:**
    - Understands context
    - Handles variations
    - Extracts complex data
    - Adapts to changes
    
    **When AI is used:**
    - Phone numbers with text
    - Complex addresses
    - Ambiguous data
    - Unusual formats
    """)

with col2:
    st.subheader("‚ö° Hybrid Approach")
    st.markdown("""
    **Best of both worlds:**
    - Fast regex for simple data
    - AI for complex cases
    - 60-80% success rate
    - Adapts automatically
    
    **Extraction methods:**
    - ü§ñ AI: Smart extraction
    - ‚ö° Regex: Fast fallback
    """)

st.markdown("---")

st.subheader("üí° Tips for Best Results")
st.markdown("""
**Prerequisites:**
1. Make sure Ollama is running: `ollama serve`
2. Llama 3.2 model installed: `ollama pull llama3.2`

**For best accuracy:**
- Start with 10 stores to test
- Enable pagination for more data
- Watch browser to see AI working
- Check "extraction_method" in results

**AI vs Non-AI:**
- AI version: 60-80% success (smarter)
- Regex version: 40-60% success (faster)
- Choose based on your needs!
""")

